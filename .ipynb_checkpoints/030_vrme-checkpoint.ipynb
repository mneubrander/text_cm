{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2016cda-e416-4c36-8c50-e15b023aaeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.neighbors import NearestNeighbors, NearestCentroid\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import ast\n",
    "import itertools\n",
    "import vrme_lib\n",
    "\n",
    "import vrme_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33604f62-6d3e-4fba-bcae-0a781b6f572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atcs(folder, embedding):\n",
    "    df_embeddings, df_submissions, df_submission_labels, df_embeddings_2017, df_embeddings_2018 = vrme_lib.load_data(path = 'data/' + folder + \"/\", embedding_path = 'data/embeddings/' + embedding)\n",
    "    \n",
    "    atc_results = []\n",
    "    \n",
    "    embedding = embedding\n",
    "    date = folder.split('_')[0] if '_' in folder else folder\n",
    "    data = folder.split('_', 1)[1] if '_' in folder else folder\n",
    "   \n",
    "    atc_results.append(embedding)\n",
    "    atc_results.append(date)\n",
    "    atc_results.append(data)\n",
    "    \n",
    "    #naive means\n",
    "    bootstrap_naive_mean = []\n",
    "\n",
    "    for n in range(5000):\n",
    "        sample_2017 = df_embeddings_2017.AVG_rating.sample(n=df_embeddings_2017.shape[0], replace = True ,random_state=n)\n",
    "        sample_2018 = df_embeddings_2018.AVG_rating.sample(n=df_embeddings_2018.shape[0], replace = True ,random_state=n)\n",
    "\n",
    "        sample_diff = sample_2018.mean() - sample_2017.mean()\n",
    "\n",
    "        bootstrap_naive_mean.append(sample_diff)\n",
    "\n",
    "    vrme_lib.print_results(name=\"Naive \",\n",
    "                        atc = df_embeddings_2018.AVG_rating.mean() - df_embeddings_2017.AVG_rating.mean(),\n",
    "                          ci_lower_bound=np.quantile(bootstrap_naive_mean, 0.025),\n",
    "                          ci_upper_bound=np.quantile(bootstrap_naive_mean, 0.975))\n",
    "    \n",
    "    atc_results.append(df_embeddings_2018.AVG_rating.mean() - df_embeddings_2017.AVG_rating.mean())\n",
    "    atc_results.append(np.quantile(bootstrap_naive_mean, 0.025))\n",
    "    atc_results.append(np.quantile(bootstrap_naive_mean, 0.975))\n",
    "    \n",
    "    df_embeddings, df_submissions, df_submission_labels, df_embeddings_2017, df_embeddings_2018 = vrme_lib.load_data(path = 'data/' + folder + \"/\", embedding_path = 'data/embeddings/' + embedding)\n",
    "    \n",
    "\n",
    "    flipped = 0\n",
    "    # Check the number of rows in df_embeddings_2017 and df_embeddings_2018\n",
    "    if len(df_embeddings_2017) > len(df_embeddings_2018):\n",
    "        # Function to swap 2017 and 2018 in conf_year column\n",
    "        def swap_conf_year(df):\n",
    "            df_copy = df.copy()\n",
    "            temp_value = -1  # Temporary placeholder value\n",
    "            df_copy.loc[df_copy['conf_year'] == 2017, 'conf_year'] = temp_value\n",
    "            df_copy.loc[df_copy['conf_year'] == 2018, 'conf_year'] = 2017\n",
    "            df_copy.loc[df_copy['conf_year'] == temp_value, 'conf_year'] = 2018\n",
    "            return df_copy\n",
    "\n",
    "        # Apply the function to all dataframes\n",
    "        df_submissions = swap_conf_year(df_submissions)\n",
    "        df_submission_labels = swap_conf_year(df_submission_labels)\n",
    "        df_embeddings_2017 = swap_conf_year(df_embeddings_2017)\n",
    "        df_embeddings_2018 = swap_conf_year(df_embeddings_2018)\n",
    "\n",
    "        # Switch the names of df_embeddings_2017 and df_embeddings_2018\n",
    "        df_embeddings_2017, df_embeddings_2018 = df_embeddings_2018, df_embeddings_2017\n",
    "\n",
    "        #indicate that we flipped the dfs\n",
    "        flipped = 1\n",
    "\n",
    "\n",
    "    HYPERPARAM_a_max_dist_threshold = 0.1\n",
    "\n",
    "    x = np.array(df_embeddings_2017.embedding.tolist())\n",
    "    clustering = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, metric = 'cosine', distance_threshold=HYPERPARAM_a_max_dist_threshold, linkage=\"average\").fit(x)\n",
    "    df_embeddings_2017['agg_cluster'] = clustering.labels_.tolist()\n",
    "\n",
    "    #get new max cosine hyperparam\n",
    "    num = vrme_hyperparam.find_max_cosine(df_embeddings_2017, df_embeddings_2018, clustering, df_submission_labels)\n",
    "\n",
    "    HYPERPARAM_k_num_neighbors = 10\n",
    "\n",
    "    # NEW PARAM FOUND ABOVE - ENSURE CAUSAL OVERLAP\n",
    "    HYPERPARAM_b_max_cosine = num\n",
    "    \n",
    "\n",
    "    #setting up KNN for 2018\n",
    "    neigh = NearestNeighbors(n_neighbors=HYPERPARAM_k_num_neighbors, metric = 'cosine', radius = 0.3)\n",
    "    non_anchor_embedding_2018 = np.array(df_embeddings_2018.embedding.to_list())\n",
    "    neigh.fit(non_anchor_embedding_2018)\n",
    "\n",
    "    #setting up closest centroid for anchor group 2017\n",
    "    anchor_embedding_2017 = np.array(df_embeddings_2017.embedding.tolist())\n",
    "    anchor_agg_clusters_2017 = np.array(df_embeddings_2017.agg_cluster.tolist())\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(anchor_embedding_2017, anchor_agg_clusters_2017)\n",
    "\n",
    "\n",
    "    #dictionary of all the agg clusters and the 10 KNN from 2018\n",
    "    dict_agg_cluster_matches ={}\n",
    "    for cluster_id in np.unique(clustering.labels_):\n",
    "\n",
    "        distances, indices = neigh.kneighbors([clf.centroids_[cluster_id]])\n",
    "        df_anchor_embedding = pd.concat([pd.DataFrame(data = distances.T,columns =['cos_dist']),pd.DataFrame(indices.T,columns=['indices'])],axis=1)\n",
    "\n",
    "        #get all the specified cosine distance 2018 papers\n",
    "        #tuple of (dataframe of 2018 matched papers, cosine distances)\n",
    "        dict_agg_cluster_matches[cluster_id] = (\n",
    "            df_embeddings_2018.iloc[df_anchor_embedding[df_anchor_embedding['cos_dist']<= HYPERPARAM_b_max_cosine].indices.to_list(), :],\n",
    "            df_anchor_embedding[df_anchor_embedding['cos_dist']<= HYPERPARAM_b_max_cosine].cos_dist.to_list()\n",
    "        )\n",
    "\n",
    "\n",
    "    def lambda_get_2018_matches(row):\n",
    "        #get embedding matches from 2018 papers\n",
    "        #returning relevant information\n",
    "        df_clustered_papers = dict_agg_cluster_matches[row.agg_cluster]\n",
    "        lst_paper_titles = df_clustered_papers[0].title.tolist()\n",
    "        lst_paper_ids = df_clustered_papers[0].paper_id.tolist()\n",
    "        ls_paper_keywords = df_clustered_papers[0].keywords.values.tolist()\n",
    "        ls_cos_distances = df_clustered_papers[1]\n",
    "\n",
    "        return lst_paper_titles, ls_paper_keywords, lst_paper_ids, ls_cos_distances\n",
    "\n",
    "    def get_num_knn_matches(row):\n",
    "        return(len(row.titles_2018))\n",
    "\n",
    "    df_embeddings_2017[['titles_2018','keywords_2018','id_2018','cos_dist_2018']]= df_embeddings_2017.apply(lambda x: lambda_get_2018_matches(x),axis=1, result_type ='expand')\n",
    "    df_embeddings_2017['num_knn_matches'] = df_embeddings_2017.apply(lambda x: get_num_knn_matches(x),axis =1)\n",
    "\n",
    "    assert df_embeddings_2017.shape[0] == df_submission_labels[df_submission_labels['conf_year']==2017].shape[0]\n",
    "\n",
    "    #df_embeddings_2017.num_knn_matches.unique()\n",
    "\n",
    "    df_embeddings_2017['match_ave_rating'] = df_embeddings_2017.apply(lambda row: vrme_lib.lambda_get_match_potential_outcomes(row, df_embeddings_2018), axis =1)\n",
    "    df_embeddings_2017['diff_2018_2017'] = df_embeddings_2017['match_ave_rating'] - df_embeddings_2017['AVG_rating']\n",
    "\n",
    "    df_embeddings_2017 = df_embeddings_2017.loc[df_embeddings_2017['match_ave_rating'].notnull(),]\n",
    "\n",
    "    # Eqn 8\n",
    "    KNN_ATT = (df_embeddings_2017['match_ave_rating'] - df_embeddings_2017['AVG_rating']).sum()/df_embeddings_2017.shape[0]\n",
    "    assert df_embeddings_2017['match_ave_rating'].shape[0] == df_embeddings_2017['AVG_rating'].shape[0]\n",
    "\n",
    "    #bootstrap KNN confidence interval\n",
    "    bootstrap_mean = []\n",
    "    for n in range(5000):\n",
    "        sample = df_embeddings_2017.diff_2018_2017.sample(n=df_embeddings_2017.shape[0], replace = True ,random_state=n)\n",
    "        bootstrap_mean.append(sample.mean())\n",
    "\n",
    "    if(flipped != 0):\n",
    "        vrme_lib.print_results(name=\"VRM-E\",\n",
    "                              atc=-statistics.mean(bootstrap_mean),\n",
    "                              ci_upper_bound=-np.quantile(bootstrap_mean, 0.025),\n",
    "                              ci_lower_bound=-np.quantile(bootstrap_mean, 0.975))\n",
    "        atc_results.append(-statistics.mean(bootstrap_mean))\n",
    "        atc_results.append(-np.quantile(bootstrap_mean, 0.975))\n",
    "        atc_results.append(-np.quantile(bootstrap_mean, 0.025))\n",
    "\n",
    "    else:\n",
    "        vrme_lib.print_results(name=\"VRM-E\",\n",
    "                              atc=statistics.mean(bootstrap_mean),\n",
    "                              ci_lower_bound=np.quantile(bootstrap_mean, 0.025),\n",
    "                              ci_upper_bound=np.quantile(bootstrap_mean, 0.975))\n",
    "        \n",
    "        atc_results.append(statistics.mean(bootstrap_mean))\n",
    "        atc_results.append(np.quantile(bootstrap_mean, 0.025))\n",
    "        atc_results.append(np.quantile(bootstrap_mean, 0.975))\n",
    "        \n",
    "    atc_results.append(HYPERPARAM_b_max_cosine)\n",
    "    return atc_results\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc66fd0-c8f3-4834-b9f7-242b358d47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\"df_embeddings.csv\", \"doc2vec100_embeddings.csv\", \"doc2vec1000_embeddings.csv\", \"bow_embeddings.csv\"]\n",
    "\n",
    "dates = [\"reviewer\" , \"rebuttal\", \"decision\"]\n",
    "\n",
    "data = [\"all_2018\", \"found_2018\", \"all_2017_found_2018\", \"all_2017_2018\"]\n",
    "\n",
    "for embedding in embeddings:\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(embedding)\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    for date in dates:\n",
    "        for data_s in data:\n",
    "            folder = date + \"_\" + data_s\n",
    "            print(\"**\" + folder + \"**\")\n",
    "            get_atcs(folder, embedding)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee92a7-b3f6-4f5f-8d62-2e0438c945ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
